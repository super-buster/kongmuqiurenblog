---
title: "PID优化器"
date: 2021-07-31T11:09:38+08:00
draft: false
math: true
---

上个星期在回顾优化器的时候注意到动量法和物联网控制中的一个控制算法PID很像，并且很多优化器可以看作是PID的一种特殊形式。突发奇想能不能把这个算法用在优化器中。之前大概是我大三的时候在实验课上用matlab做过PID的实验，但是具体怎么实现早就忘了，于是乎在一番google下做了一个PID控制的DEMO。代码如下:
#+begin_src python :results output
# coding: utf-8
from collections import deque
import random


class PID(object):
    def __init__(self, sp):
        super(PID).__init__()
        self.erros = deque(maxlen=5)
        self.erros.extend([0,0,0,0,0])
        self.sp = sp
        self.pv = 3

    @property
    def measure(self):
        return self.pv

    @measure.setter
    def controll(self,v):
        self.pv=v

    def oscillation(self):
        self.pv+=random.normalvariate(0,1)

    def compute(self, Kp, Ki, Kd, ts):
        erro = self.sp-self.pv
        self.erros.append(erro)
        pterm = Kp*erro
        iterm = Ki*sum(self.erros)/len(self.erros)
        dterm = Kd*(self.erros[-1]-self.erros[-2])
        out = pterm+iterm+dterm
        return out


def main():
    timestep=10
    sample_num=100
    pid=PID(50)
    Kp,Ki,Kd=0.1,0.2,0.1
    speeds=[]
    #last_input=3
    for t in range(timestep):
        for s in range(sample_num):
            pid.oscillation()
            pid.controll=pid.compute(Kp,Ki,Kd,1/sample_num)+pid.measure
            #last_input=pid.measure
            speeds.append(pid.measure)
    print(speeds[:15])
main()
#+end_src
在确认过这个算法的有效性后，我尝试用De Jong的优化函数来测试一番：
#+begin_src python :results output
  lr=0.4
  gamma=0.6
  epsilon=1e-6
  Kd=0.03
  Ki=0.1
  Kp=0.1

  def train_2d(trainer):
      x1,x2,s1,s2=-5,-2,[0,0,0],[0,0,0]
      results=[(x1,x2)]
      for i in range(20):
          x1,x2,s1,s2=trainer(x1,x2,s1,s2)
          results.append((x1,x2))
      print('epoch {}, x1 {} x2 {}'.format(i+1,x1,x2))
      return results

  def pid_2d(x1,x2,s1,s2):
      s1=[(0.2*x1),(1-gamma)*(0.2*x1)**2+gamma*s1[0],(0.2*x1)-s1[0]]
      s2=[(4*x2),(1-gamma)*(4*x2)**2+gamma*s2[0],(4*x2)-s2[0]]
      x1-=(lr*(1+Kp)*s1[0]+Ki*s1[1]+Kd*s1[2])
      x2-=(lr*(1+Kp)*s2[0]+Ki*s2[1]+Kd*s2[2])
      return x1,x2,s1,s2
  def show_trace_2d(f,results): # 目标函数的等值图
      # 第一个*是为了把zip后的结果（两个list）提取出来，第二个*是为了把列表中所有元素提取出来
      plt.plot(*zip(*results),'-o',color='#ff7f0e')
      # 利用两个坐标产生一个网格，返回的二维坐标是从下到上，从左到右的点
      x1,x2=np.meshgrid(np.arange(-5.5,1.0,0.1),np.arange(-3.0,2.5,0.1))
      a=plt.contourf(x1,x2,f(x1,x2),5,cmap=plt.cm.Set2,extend='both')
      plt.colorbar(a)
      #plt.clabel(a,inline=True,fontsize=18)
      plt.xlabel('x1')
      plt.ylabel('x2')
      plt.show()

  show_trace_2d(f_2d,train_2d(pid_2d))
#+end_src
结果如下：
[[/PID.png]]

得到这个结果之后我感觉这个方法确实具有可行性，后面只需要把这个写成pytorch的优化器，跑一跑深度神经网络进一步验证就OK了。但是很遗憾，我在github上搜Optimizer想要参考一下别人的代码，发现了[[https://github.com/jettify/pytorch-optimizer][这个]]。里面居然有一个叫做PID的优化器，而且好像是CVPR2018的论文。后面我看了一下它的思想和我的一样，只不过我的理解积分，微分项的系数是超参，别人论文里面是把微分项的用拉普拉斯逆变换算出来了。虽然有点沮丧，自己花了一个星期的时间做出来的东西结果早就有了，但是后面还是去看了一下它的代码，发现其实它代码里参数还是自己调的，并且我用它的代码在minist上去跑Lenet发现效果太差，也可能是参数没调对根本就收敛不了，和论文里说的效果不一样。下面讲一下它的思想,以动量法为例,它的参数更新公式如下:

\begin{equation} \left\{ \begin{aligned}& v_{t}=\beta v_{t-1} + \eta g_t  \quad (1) \\ 
& \theta_t= \theta_{t-1}-v_t \quad (2)
\end{aligned}\right.\end{equation}

把第(2)项展开得到

\begin{equation}
\theta_t= \theta_{t-1} - \eta g_t- \eta \sum_{r=1}^t \beta^r g_{t-r}
\end{equation}


训练一个神经网络最常见的算法是梯度下降法，即让损失到极小值点，此时参数梯度为0。因此第t个时刻梯度的误差就是g_t。从展开式可以看出，参数的更新和当前梯度误差，过去所有梯度误差都有关，因此动量法可以看作是一个PI模型。同理，再对NAG法分析。

\begin{equation} \left\{ \begin{aligned}& v_{t}=\gamma v_{t-1}+ \eta \triangledown_{\theta}J(\theta_{t-1}-\gamma v_{t-1}) \quad (1) \\ 
& \theta_t= \theta_{t-1}-v_t \quad (2)
\end{aligned}\right.\end{equation}

令\theta_{t-1}-\gamma v_{t-1}=\dot{\theta_{t-1}}。NAG法可以变为如下

\begin{equation} \left\{ \begin{aligned}& v_{t}=\gamma v_{t-1}+ \eta \triangledown_{\dot{\theta}}J(\dot{\theta_{t-1}}) \quad (1) \\ 
& \dot{ \theta_t }= \dot{ \theta_{t-1}}- (1+\gamma)v_{t-1} + \gamma v_{t-2}\quad (2)
\end{aligned}\right.\end{equation}

同理把（2）展开得到

\begin{equation}
\dot{\theta_{t}}=\dot{\theta_{t-1}}-(1+\gamma)\eta \triangledown_{\dot{\theta}}J(\dot{\theta}_{t-1})- \eta \sum_{r=2}^{t} \gamma^r J(\dot{\theta}_{t-r})
\end{equation}


可以发现NAG也是PI模型，只不过P参数更大了。

总结：这一个星期还是有收获的，首先明白了一个简单的想法很可能有人已经做过了，因此自己动手前最好可以先自习查一下。第二，学会了怎么在pytorch中实现自己的优化器。
